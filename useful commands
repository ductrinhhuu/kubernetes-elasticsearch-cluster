# reset kubeadm
echo "y" | sudo kubeadm reset


# remove old config
sudo rm -r ~/.kube


# init kubernetes (use flannel)
sudo kubeadm init --pod-network-cidr=10.244.0.0/16


# init pod (here I use flannel)
kubectl apply -f ~/kube-flannel.yml


# create service/storage/pods from config file (yaml/yml)
kubectl create -f <file-name>


# delete service/storage/pods by id
kubectl delete <service-name> <id>


# delete service/storage/pods by file
kubectl delete -f <file-name>


# get all running stuff
kubectl get pv,pvc,pod,service,storageclass


# describe detail of service
kubectl describe <service-name> <id>


# see log of a pod of master
kubectl logs -f pod/<pod-id> | grep ClusterApplierService


####### add label to a node
# get nodes info
kubectl get nodes

# labeling your nodes
kubectl label nodes <your-node-name> disktype=ssd

# show nodes info with label
kubectl get nodes --show-labels

# then specify node selector in config
...
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
...
















############## NOTE #################
Create Persistent Volume for each of nodes running
e.g: If Elasticsearch has 3 master nodes and 1 data node -> create 4 Persistent Volume